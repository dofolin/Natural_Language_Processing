# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EC7Zw0o0NF6qSjsOqariKkRb-xjDoM-m

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.

# processed data storage
processed_data = []

# subcategory storage
current_subcategory = ""
subcategory_count = 0

# category storage
categories = ["semantic", "syntatic"]

for row in data:
    # if a row starts with ":" , it's a subcategory's name
    if row.startswith(":"):
        current_subcategory = row[2:]  # remove ": " and extract subcategory's name
        subcategory_count += 1  # number of subcategory count
    else:
        # by number of subcategory count to separate semantic and syntatic(total:14 semantic:5 syntatic:9)
        if subcategory_count <= 5:
            category = "semantic"
        else:
            category = "syntatic"

        # data storage
        processed_data.append({
            "Question": row,  # string
            "Category": category,  # semantic or syntatic
            "SubCategory": current_subcategory  # current subcategory
        })

df = pd.DataFrame(processed_data)

print(df.head())

# Create the dataframe
# df = pd.DataFrame(
#     {
#         "Question": questions,
#         "Category": categories,
#         "SubCategory": sub_categories,
#     }
# )

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

data

MODEL_NAME = "word2vec-google-news-300"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      #""" Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      #"""
      #以下使用chatGPT:
          # 解析每個類比問題，假設格式為 "word_a word_b word_c word_d"
    words = analogy.split()
    if len(words) != 4:
        continue  # 確保每個類比問題有四個詞

    word_a, word_b, word_c, word_d = words  # 解包類比問題

    #try:
        # 使用詞嵌入進行向量運算，預測 word_d (即第四個詞)
    predicted_word = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]
    preds.append(predicted_word)  # 儲存預測結果
    golds.append(word_d)  # 儲存正確答案（即 gold answer）
    #except KeyError:
        # 如果詞在詞嵌入模型中不存在，跳過這個類比問題
    #    continue

len(preds)

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
# 以下使用chatGPT生成
# 假設 data 包含一個 "SubCategory" 欄位，並且我們只想選擇 "family" 子類別的 preds
family_mask = data["SubCategory"] == "family"
family_preds = [preds[i] for i in range(len(preds)) if family_mask[i]]  # 根據 family 子類別過濾 preds

# 去重處理，確保每個詞只出現一次
family_preds = list(set(family_preds))

# 檢查是否有足夠的預測結果
if len(family_preds) == 0:
    print("No predicted words found for the 'family' subcategory.")
else:
    # 獲取這些 family_preds 的詞向量
    family_preds_vectors = []
    valid_family_preds = []
    missing_family_preds = []

    for word in family_preds:
        word = word.lower()  # 確保詞是小寫
        try:
            family_preds_vectors.append(model[word])  # 獲取詞的向量
            valid_family_preds.append(word)  # 記錄有效的預測詞
        except KeyError:
            missing_family_preds.append(word)  # 記錄不在模型中的詞

    # 如果有缺失詞，打印它們
    if missing_family_preds:
        print(f"Missing predicted words in 'family' subcategory: {missing_family_preds}")

    # 將詞向量轉換為 NumPy 陣列
    family_preds_vectors = np.array(family_preds_vectors)

    # 檢查是否有足夠的樣本進行 t-SNE
    n_samples = family_preds_vectors.shape[0]
    if n_samples == 0:
        print("No valid predicted words found for t-SNE visualization in 'family' subcategory.")
    else:
        # 設置 perplexity，確保小於樣本數量並且至少為 1
        perplexity_value = max(1, min(30, n_samples // 2))

        # 使用 t-SNE 將高維向量投影到 2D
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
        family_preds_vectors_2d = tsne.fit_transform(family_preds_vectors)

        # 畫出 t-SNE 結果
        plt.figure(figsize=(10, 10))
        for i, word in enumerate(valid_family_preds):
            plt.scatter(family_preds_vectors_2d[i, 0], family_preds_vectors_2d[i, 1])
            plt.text(family_preds_vectors_2d[i, 0] + 0.02, family_preds_vectors_2d[i, 1] + 0.02, word, fontsize=12)

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz
!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz
!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz
!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz
!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz

# Download the split Wikipedia files
# Each file contain 562365 lines (articles), except the last file.
!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz
!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz
!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz
!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz
!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz
!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz

#from google.colab import drive
#drive.mount('/content/drive/')

#!gunzip -k /content/drive/MyDrive/Data_Colab/wiki_texts_part_*.gz

#!cat /content/drive/MyDrive/Data_Colab/wiki_texts_part_*.txt > wiki_texts_combined.txt

# Extract the downloaded wiki_texts_parts files.
!gunzip -k wiki_texts_part_*.gz

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random


wiki_txt_path = "wiki_texts_combined.txt"
output_path = "wiki_texts_sampled20.txt"


sample_ratio = 0.2


with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
        for line in f:
            # random.random() == 0~1
            if random.random() < sample_ratio:
                output_file.write(line)

print(f"已完成按比例隨機取樣，結果保存在 {output_path} 文件中。")

!head -n 10 wiki_texts_sampled20.txt

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
import re
import string
import nltk
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

# 以下使用chatGPT
# 下載 NLTK 的 stopwords 資料
nltk.download('stopwords')

# 定義停用詞和標點符號
stop_words = set(stopwords.words('english'))
punctuation = string.punctuation

# 檢查文件路徑
input_file = "wiki_texts_sampled20.txt"
output_file = "wiki_texts_processed.txt"
# 預處理函數
def preprocess_text(text):
    # 移除標點符號和特殊字符
    text = re.sub(r'[^\w\s]', '', text)


    # 分詞，並移除停用詞和單字符
    words = text.split()
    words = [word for word in words if word not in stop_words and len(word) > 1]

    return words


# 逐行讀取並預處理文本，將結果直接寫入磁碟
with open(input_file, "r", encoding="utf-8") as f, open(output_file, "w", encoding="utf-8") as out_f:
    for line in f:
        # 預處理每行文本
        processed_line = preprocess_text(line)
        if processed_line:  # 只保留有內容的句子
            out_f.write(" ".join(processed_line) + "\n")  # 將預處理結果寫入文件

print(f"預處理完成，結果保存在 {output_file} 文件中。")

input_file = "wiki_texts_processed.txt"

sentences = LineSentence(input_file)

# Word2Vec
model = Word2Vec(sentences=sentences, vector_size=300, window=10, min_count=1, workers=4)

# model save
model.save("wiki_word2vec.model")

print("Word2Vec 模型已訓練並保存。")

# Google Drive
#model_path = "/content/drive/MyDrive/wiki_word2vec.model"
#model.save(model_path)

#print(f"模型已保存到 Google Drive: {model_path}")

#model = Word2Vec.load("/content/drive/MyDrive/wiki_word2vec.model")

model = Word2Vec.load("wiki_word2vec.model")

model.wv.save("wiki_word2vec.kv")

from gensim.models import KeyedVectors

model = KeyedVectors.load("wiki_word2vec.kv", mmap='r')

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []
processed_mask = []  # mask word

for analogy in tqdm(data["Question"]):
      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      #""" Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      #"""
                # 解析每個類比問題，假設格式為 "word_a word_b word_c word_d"
    words = analogy.split()
    if len(words) != 4:
        processed_mask.append(False)  # fail
        continue

    word_a, word_b, word_c, word_d = words

    try:
        # predict
        predicted_word = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]
        preds.append(predicted_word)  # pred answer
        golds.append(word_d)  #  gold answer
        processed_mask.append(True)  # mask
    except KeyError as e:
        print(f"Skipping analogy due to missing word: {e}")
        processed_mask.append(False)  # fail
        continue  # skip NA


golds_full = [""] * len(data)
preds_full = [""] * len(data)


j = 0
for i, processed in enumerate(processed_mask):
    if processed:
        golds_full[i] = golds[j]
        preds_full[i] = preds[j]
        j += 1


golds = golds_full
preds = preds_full

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`

# 假設 data 包含一個 "SubCategory" 欄位，並且我們只想選擇 "family" 子類別的 preds
family_mask = data["SubCategory"] == "family"
family_preds = [preds[i] for i in range(len(preds)) if family_mask[i]]  # 根據 family 子類別過濾 preds

# 去重處理，確保每個詞只出現一次
family_preds = list(set(family_preds))

# 檢查是否有足夠的預測結果
if len(family_preds) == 0:
    print("No predicted words found for the 'family' subcategory.")
else:
    # 獲取這些 family_preds 的詞向量
    family_preds_vectors = []
    valid_family_preds = []
    missing_family_preds = []

    for word in family_preds:
        word = word.lower()  # 確保詞是小寫
        try:
            family_preds_vectors.append(model[word])  # 獲取詞的向量
            valid_family_preds.append(word)  # 記錄有效的預測詞
        except KeyError:
            missing_family_preds.append(word)  # 記錄不在模型中的詞

    # 如果有缺失詞，打印它們
    if missing_family_preds:
        print(f"Missing predicted words in 'family' subcategory: {missing_family_preds}")

    # 將詞向量轉換為 NumPy 陣列
    family_preds_vectors = np.array(family_preds_vectors)

    # 檢查是否有足夠的樣本進行 t-SNE
    n_samples = family_preds_vectors.shape[0]
    if n_samples == 0:
        print("No valid predicted words found for t-SNE visualization in 'family' subcategory.")
    else:
        # 設置 perplexity，確保小於樣本數量並且至少為 1
        perplexity_value = max(1, min(30, n_samples // 2))

        # 使用 t-SNE 將高維向量投影到 2D
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
        family_preds_vectors_2d = tsne.fit_transform(family_preds_vectors)

        # 畫出 t-SNE 結果
        plt.figure(figsize=(10, 10))
        for i, word in enumerate(valid_family_preds):
            plt.scatter(family_preds_vectors_2d[i, 0], family_preds_vectors_2d[i, 1])
            plt.text(family_preds_vectors_2d[i, 0] + 0.02, family_preds_vectors_2d[i, 1] + 0.02, word, fontsize=12)


plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")