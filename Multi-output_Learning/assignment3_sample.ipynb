{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dofolin/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers as T\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MultiLabelModel().to(device)\n",
    "tokenizer = T.BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:08:20.001043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-20 20:08:20.001177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-20 20:08:20.231453: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 20:08:20.713873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-20 20:08:25.227423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/dofolin/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dofolin/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "model = MultiLabelModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # 把中文標點替換掉\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "lr = 3e-5\n",
    "epochs = 6\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # Write your code here\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "    premises = [item['premise'] for item in batch]\n",
    "    hypotheses = [item['hypothesis'] for item in batch]\n",
    "    labels_relatedness = torch.tensor([item['relatedness_score'] for item in batch], dtype=torch.float32)\n",
    "    labels_entailment = torch.tensor([item['entailment_judgment'] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    encoding = tokenizer(premises, hypotheses, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels_relatedness': labels_relatedness,\n",
    "        'labels_entailment': labels_entailment\n",
    "    }\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "train_dataset = SemevalDataset(split=\"train\")\n",
    "validation_dataset = SemevalDataset(split=\"validation\")\n",
    "\n",
    "dl_train = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)# Write your code here\n",
    "dl_validation = DataLoader(validation_dataset, batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101,  1996,  2158,  2003,  8783,  3347, 17327,  2015,   102,  1996,\n",
      "          2158,  2003, 13845,  3347, 17327,  2015,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2502,  3869,  2003,  9105,  1037,  2158,   102,  1037,\n",
      "          2158,  2003,  9105,  1037,  3869,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2450,  2003,  2559, 16484,  2012,  1037,  2158,   102,\n",
      "          1037,  2158,  2003,  2108, 16484,  2246,  2012,  2011,  1037,  2450,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2711,  2003,  7367,  6299,  2075,  1996, 11756,  1997,\n",
      "          2019,  4064,  5898,  9573,  2007,  1037,  4690,   102,  1037,  2158,\n",
      "          2003,  6276,  1037,  9573,  2007,  1037,  4690,   102],\n",
      "        [  101,  1996,  2158,  2003,  2652,  1996,  2858,   102,  1996,  2711,\n",
      "          2003,  2652,  1996,  2858,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2619,  2003,  3173,  1037, 17834, 25852,   102,  2619,  2003,\n",
      "          3173,  1037,  2235,  4111,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2158,  2003,  8660,  3031,  1037,  2659,  2813,   102,\n",
      "          1037,  2158,  2003,  3061,  1999,  2392,  1997,  1037,  2813,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  2450,  2003, 26514, 20548,   102,  1037,  2450,  2003,\n",
      "          2025, 26514,  2019, 20949,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "Relatedness Labels: tensor([4.0000, 2.6000, 4.4000, 4.0000, 4.5000, 4.4500, 3.0000, 3.4000])\n",
      "Entailment Labels: tensor([0, 0, 1, 1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# check the first batch:\n",
    "for batch in dl_train:\n",
    "    print(\"Input IDs:\", batch['input_ids'])\n",
    "    print(\"Attention Mask:\", batch['attention_mask'])\n",
    "    print(\"Relatedness Labels:\", batch['labels_relatedness'])\n",
    "    print(\"Entailment Labels:\", batch['labels_entailment'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Write your code here\n",
    "        # Define what modules you will use in the model\n",
    "        super(MultiLabelModel, self).__init__()\n",
    "        # bert-base-uncased\n",
    "        self.bert = T.BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        # two layers\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 1)  # reg.\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 3)  # cls 3.\n",
    "    def forward(self, **kwargs):\n",
    "        # Write your code here\n",
    "        # Forward pass\n",
    "        # Use BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # BERT pooling\n",
    "        \n",
    "        # output\n",
    "        relatedness_score = self.regressor(pooled_output).squeeze(-1)  # minus dimension\n",
    "        entailment_logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return relatedness_score, entailment_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dofolin/anaconda3/envs/myenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)# Write your code here\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# Write your code here\n",
    "loss_fn_relatedness = torch.nn.MSELoss()  # reg. loss\n",
    "loss_fn_entailment = torch.nn.CrossEntropyLoss()  # cls. loss\n",
    "\n",
    "# scoring functions\n",
    "spc = SpearmanCorrCoef().to(device)\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3, average='macro').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/6]: 100%|██████████| 563/563 [00:14<00:00, 38.46it/s]\n",
      "Validation epoch [1/6]: 100%|██████████| 63/63 [00:01<00:00, 56.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [1/6]:\n",
      "  Loss (Relatedness): 0.2830\n",
      "  Loss (Entailment): 0.5626\n",
      "  Spearman Correlation: 0.7630\n",
      "  Accuracy: 0.8611\n",
      "  F1 Score: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/6]: 100%|██████████| 563/563 [00:14<00:00, 38.35it/s]\n",
      "Validation epoch [2/6]: 100%|██████████| 63/63 [00:01<00:00, 54.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [2/6]:\n",
      "  Loss (Relatedness): 0.2839\n",
      "  Loss (Entailment): 0.5263\n",
      "  Spearman Correlation: 0.7526\n",
      "  Accuracy: 0.8631\n",
      "  F1 Score: 0.8362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/6]: 100%|██████████| 563/563 [00:14<00:00, 38.50it/s]\n",
      "Validation epoch [3/6]: 100%|██████████| 63/63 [00:01<00:00, 44.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [3/6]:\n",
      "  Loss (Relatedness): 0.2989\n",
      "  Loss (Entailment): 0.6112\n",
      "  Spearman Correlation: 0.7591\n",
      "  Accuracy: 0.8611\n",
      "  F1 Score: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [4/6]: 100%|██████████| 563/563 [00:14<00:00, 38.43it/s]\n",
      "Validation epoch [4/6]: 100%|██████████| 63/63 [00:01<00:00, 49.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [4/6]:\n",
      "  Loss (Relatedness): 0.2756\n",
      "  Loss (Entailment): 0.5931\n",
      "  Spearman Correlation: 0.7497\n",
      "  Accuracy: 0.8571\n",
      "  F1 Score: 0.8011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [5/6]: 100%|██████████| 563/563 [00:14<00:00, 38.47it/s]\n",
      "Validation epoch [5/6]: 100%|██████████| 63/63 [00:01<00:00, 47.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [5/6]:\n",
      "  Loss (Relatedness): 0.2850\n",
      "  Loss (Entailment): 0.5570\n",
      "  Spearman Correlation: 0.7305\n",
      "  Accuracy: 0.8611\n",
      "  F1 Score: 0.8065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [6/6]: 100%|██████████| 563/563 [00:14<00:00, 38.41it/s]\n",
      "Validation epoch [6/6]: 100%|██████████| 63/63 [00:01<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [6/6]:\n",
      "  Loss (Relatedness): 0.2998\n",
      "  Loss (Entailment): 0.5605\n",
      "  Spearman Correlation: 0.7314\n",
      "  Accuracy: 0.8671\n",
      "  F1 Score: 0.8183\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()  # remove grad.\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "        labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        relatedness_score, entailment_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "        loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "        loss = loss_relatedness + loss_entailment\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    total_loss_relatedness = 0\n",
    "    total_loss_entailment = 0\n",
    "    total_spc = 0\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "            labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            relatedness_score, entailment_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "            loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "            total_loss_relatedness += loss_relatedness.item()\n",
    "            total_loss_entailment += loss_entailment.item()\n",
    "\n",
    "            # Compute metrics\n",
    "            total_spc += spc(relatedness_score, labels_relatedness).item()\n",
    "            total_acc += acc(entailment_logits, labels_entailment).item()\n",
    "            total_f1 += f1(entailment_logits, labels_entailment).item()\n",
    "            num_batches += 1\n",
    "    # Evaluate your model\n",
    "    avg_loss_relatedness = total_loss_relatedness / num_batches\n",
    "    avg_loss_entailment = total_loss_entailment / num_batches\n",
    "    avg_spc = total_spc / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    avg_f1 = total_f1 / num_batches\n",
    "\n",
    "    print(f\"Validation Results - Epoch [{ep+1}/{epochs}]:\")\n",
    "    print(f\"  Loss (Relatedness): {avg_loss_relatedness:.4f}\")\n",
    "    print(f\"  Loss (Entailment): {avg_loss_entailment:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {avg_spc:.4f}\")\n",
    "    print(f\"  Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"  F1 Score: {avg_f1:.4f}\")\n",
    "    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "    torch.save(model, f'./saved_models/ep{ep}.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test set predictions, you can write perform evaluation simlar to #TODO5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SemevalDataset(split=\"test\")\n",
    "dl_test = DataLoader(test_dataset, batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set predictions: 100%|██████████| 616/616 [00:16<00:00, 36.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Results:\n",
      "  Loss (Relatedness): 0.2934\n",
      "  Loss (Entailment): 0.6168\n",
      "  Spearman Correlation: 0.7144\n",
      "  Accuracy: 0.8610\n",
      "  F1 Score: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(f\"Test set predictions\")\n",
    "model.eval()\n",
    "total_loss_relatedness = 0\n",
    "total_loss_entailment = 0\n",
    "total_spc = 0\n",
    "total_acc = 0\n",
    "total_f1 = 0\n",
    "num_batches = 0\n",
    "with torch.no_grad():\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "        labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        relatedness_score, entailment_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "        loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "        total_loss_relatedness += loss_relatedness.item()\n",
    "        total_loss_entailment += loss_entailment.item()\n",
    "\n",
    "        # Compute metrics\n",
    "        total_spc += spc(relatedness_score, labels_relatedness).item()\n",
    "        total_acc += acc(entailment_logits, labels_entailment).item()\n",
    "        total_f1 += f1(entailment_logits, labels_entailment).item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Output test set evaluation scores\n",
    "avg_loss_relatedness = total_loss_relatedness / num_batches\n",
    "avg_loss_entailment = total_loss_entailment / num_batches\n",
    "avg_spc = total_spc / num_batches\n",
    "avg_acc = total_acc / num_batches\n",
    "avg_f1 = total_f1 / num_batches\n",
    "\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"  Loss (Relatedness): {avg_loss_relatedness:.4f}\")\n",
    "print(f\"  Loss (Entailment): {avg_loss_entailment:.4f}\")\n",
    "print(f\"  Spearman Correlation: {avg_spc:.4f}\")\n",
    "print(f\"  Accuracy: {avg_acc:.4f}\")\n",
    "print(f\"  F1 Score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with models trained separately on each of the sub-task, does multi-output learning improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelatednessModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelatednessModel, self).__init__()\n",
    "        self.bert = T.BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 1)  # Regression output\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        relatedness_score = self.regressor(pooled_output).squeeze(-1)\n",
    "        return relatedness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedness_model = RelatednessModel().to(device)\n",
    "optimizer_relatedness = AdamW(relatedness_model.parameters(), lr=lr)\n",
    "loss_fn_relatedness = torch.nn.MSELoss()\n",
    "spc = SpearmanCorrCoef().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [1/6]: 100%|██████████| 563/563 [00:18<00:00, 29.75it/s]\n",
      "Validation Relatedness Model epoch [1/6]: 100%|██████████| 63/63 [00:01<00:00, 45.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [1/6]:\n",
      "  Loss (Relatedness): 0.3727\n",
      "  Spearman Correlation: 0.7411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [2/6]: 100%|██████████| 563/563 [00:15<00:00, 35.88it/s]\n",
      "Validation Relatedness Model epoch [2/6]: 100%|██████████| 63/63 [00:00<00:00, 102.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [2/6]:\n",
      "  Loss (Relatedness): 0.4256\n",
      "  Spearman Correlation: 0.7421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [3/6]: 100%|██████████| 563/563 [00:15<00:00, 35.85it/s]\n",
      "Validation Relatedness Model epoch [3/6]: 100%|██████████| 63/63 [00:00<00:00, 93.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [3/6]:\n",
      "  Loss (Relatedness): 0.2848\n",
      "  Spearman Correlation: 0.7463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [4/6]: 100%|██████████| 563/563 [00:15<00:00, 36.09it/s]\n",
      "Validation Relatedness Model epoch [4/6]: 100%|██████████| 63/63 [00:00<00:00, 86.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [4/6]:\n",
      "  Loss (Relatedness): 0.3410\n",
      "  Spearman Correlation: 0.7098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [5/6]: 100%|██████████| 563/563 [00:15<00:00, 35.89it/s]\n",
      "Validation Relatedness Model epoch [5/6]: 100%|██████████| 63/63 [00:00<00:00, 80.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [5/6]:\n",
      "  Loss (Relatedness): 0.2582\n",
      "  Spearman Correlation: 0.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Relatedness Model epoch [6/6]: 100%|██████████| 563/563 [00:15<00:00, 36.05it/s]\n",
      "Validation Relatedness Model epoch [6/6]: 100%|██████████| 63/63 [00:00<00:00, 75.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Relatedness Model - Epoch [6/6]:\n",
      "  Loss (Relatedness): 0.2934\n",
      "  Spearman Correlation: 0.7469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training Relatedness Model epoch [{ep+1}/{epochs}]\")\n",
    "    relatedness_model.train()\n",
    "    for batch in pbar:\n",
    "        optimizer_relatedness.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        relatedness_score = relatedness_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "        loss_relatedness.backward()\n",
    "        optimizer_relatedness.step()\n",
    "\n",
    "    # Validation loop\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation Relatedness Model epoch [{ep+1}/{epochs}]\")\n",
    "    relatedness_model.eval()\n",
    "    total_loss_relatedness = 0\n",
    "    total_spc = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            relatedness_score = relatedness_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "            total_loss_relatedness += loss_relatedness.item()\n",
    "\n",
    "            # Compute metric\n",
    "            total_spc += spc(relatedness_score, labels_relatedness).item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss_relatedness = total_loss_relatedness / num_batches\n",
    "    avg_spc = total_spc / num_batches\n",
    "    print(f\"Validation Relatedness Model - Epoch [{ep+1}/{epochs}]:\")\n",
    "    print(f\"  Loss (Relatedness): {avg_loss_relatedness:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {avg_spc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntailmentModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EntailmentModel, self).__init__()\n",
    "        self.bert = T.BertModel.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 3)  # Classification output\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        entailment_logits = self.classifier(pooled_output)\n",
    "        return entailment_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_model = EntailmentModel().to(device)\n",
    "optimizer_entailment = AdamW(entailment_model.parameters(), lr=lr)\n",
    "loss_fn_entailment = torch.nn.CrossEntropyLoss()\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [1/6]: 100%|██████████| 563/563 [00:14<00:00, 38.39it/s]\n",
      "Validation Entailment Model epoch [1/6]: 100%|██████████| 63/63 [00:00<00:00, 102.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [1/6]:\n",
      "  Loss (Entailment): 0.4016\n",
      "  Accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [2/6]: 100%|██████████| 563/563 [00:14<00:00, 38.83it/s]\n",
      "Validation Entailment Model epoch [2/6]: 100%|██████████| 63/63 [00:00<00:00, 126.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [2/6]:\n",
      "  Loss (Entailment): 0.3718\n",
      "  Accuracy: 0.8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [3/6]: 100%|██████████| 563/563 [00:14<00:00, 38.00it/s]\n",
      "Validation Entailment Model epoch [3/6]: 100%|██████████| 63/63 [00:00<00:00, 126.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [3/6]:\n",
      "  Loss (Entailment): 0.4364\n",
      "  Accuracy: 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [4/6]: 100%|██████████| 563/563 [00:15<00:00, 35.79it/s]\n",
      "Validation Entailment Model epoch [4/6]: 100%|██████████| 63/63 [00:00<00:00, 126.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [4/6]:\n",
      "  Loss (Entailment): 0.4823\n",
      "  Accuracy: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [5/6]: 100%|██████████| 563/563 [00:15<00:00, 36.02it/s]\n",
      "Validation Entailment Model epoch [5/6]: 100%|██████████| 63/63 [00:00<00:00, 125.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [5/6]:\n",
      "  Loss (Entailment): 0.4726\n",
      "  Accuracy: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Entailment Model epoch [6/6]: 100%|██████████| 563/563 [00:15<00:00, 35.81it/s]\n",
      "Validation Entailment Model epoch [6/6]: 100%|██████████| 63/63 [00:00<00:00, 126.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Entailment Model - Epoch [6/6]:\n",
      "  Loss (Entailment): 0.5327\n",
      "  Accuracy: 0.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training Entailment Model epoch [{ep+1}/{epochs}]\")\n",
    "    entailment_model.train()\n",
    "    for batch in pbar:\n",
    "        optimizer_entailment.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        entailment_logits = entailment_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "        loss_entailment.backward()\n",
    "        optimizer_entailment.step()\n",
    "\n",
    "    # Validation loop\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation Entailment Model epoch [{ep+1}/{epochs}]\")\n",
    "    entailment_model.eval()\n",
    "    total_loss_entailment = 0\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            entailment_logits = entailment_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "            total_loss_entailment += loss_entailment.item()\n",
    "\n",
    "            # Compute metric\n",
    "            total_acc += acc(entailment_logits, labels_entailment).item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss_entailment = total_loss_entailment / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    print(f\"Validation Entailment Model - Epoch [{ep+1}/{epochs}]:\")\n",
    "    print(f\"  Loss (Entailment): {avg_loss_entailment:.4f}\")\n",
    "    print(f\"  Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does your model fail to correctly predict some data points? Please provide an error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/6]: 100%|██████████| 563/563 [00:14<00:00, 38.89it/s]\n",
      "Validation epoch [1/6]: 100%|██████████| 63/63 [00:01<00:00, 53.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [1/6]:\n",
      "  Loss (Relatedness): 0.2549\n",
      "  Loss (Entailment): 0.6533\n",
      "  Spearman Correlation: 0.7757\n",
      "  Accuracy: 0.8631\n",
      "  F1 Score: 0.8198\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 1037, 2829, 1998, 2317, 3899, 2003, 2770...           0   \n",
      "1  [101, 1037, 2177, 1997, 10158, 2024, 13039, 20...           0   \n",
      "2  [101, 1996, 2450, 4147, 3165, 6471, 1010, 5061...           0   \n",
      "3  [101, 1037, 2450, 2003, 2635, 2125, 1037, 1196...           2   \n",
      "4  [101, 1037, 2711, 2003, 8218, 1037, 2600, 2007...           0   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                1               4.4               4.458434  \n",
      "1                1               4.1               4.612533  \n",
      "2                2               3.3               3.549941  \n",
      "3                0               3.5               3.627561  \n",
      "4                1               4.6               4.393768  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/6]: 100%|██████████| 563/563 [00:14<00:00, 38.62it/s]\n",
      "Validation epoch [2/6]: 100%|██████████| 63/63 [00:01<00:00, 51.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [2/6]:\n",
      "  Loss (Relatedness): 0.2437\n",
      "  Loss (Entailment): 0.5948\n",
      "  Spearman Correlation: 0.7891\n",
      "  Accuracy: 0.8690\n",
      "  F1 Score: 0.8318\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 1037, 2611, 1999, 2317, 2003, 5613, 102,...           1   \n",
      "1  [101, 1037, 2829, 1998, 2317, 3899, 2003, 2770...           0   \n",
      "2  [101, 1037, 2177, 1997, 10158, 2024, 13039, 20...           0   \n",
      "3  [101, 1996, 2450, 4147, 3165, 6471, 1010, 5061...           0   \n",
      "4  [101, 1037, 2450, 2003, 2635, 2125, 1037, 1196...           2   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                0               4.9               4.188742  \n",
      "1                1               4.4               4.442097  \n",
      "2                1               4.1               4.769308  \n",
      "3                2               3.3               3.699707  \n",
      "4                0               3.5               3.553298  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/6]: 100%|██████████| 563/563 [00:14<00:00, 38.65it/s]\n",
      "Validation epoch [3/6]: 100%|██████████| 63/63 [00:01<00:00, 48.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [3/6]:\n",
      "  Loss (Relatedness): 0.2756\n",
      "  Loss (Entailment): 0.6907\n",
      "  Spearman Correlation: 0.7663\n",
      "  Accuracy: 0.8571\n",
      "  F1 Score: 0.8224\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 2274, 2336, 2024, 3061, 1999, 2392, 1997...           0   \n",
      "1  [101, 2619, 2003, 2006, 1037, 2304, 1998, 2317...           0   \n",
      "2  [101, 1037, 2611, 1999, 2317, 2003, 5613, 102,...           1   \n",
      "3  [101, 1037, 2829, 1998, 2317, 3899, 2003, 2770...           0   \n",
      "4  [101, 1037, 2177, 1997, 10158, 2024, 13039, 20...           0   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                1             4.200               4.648133  \n",
      "1                1             3.165               4.323132  \n",
      "2                0             4.900               4.160884  \n",
      "3                1             4.400               4.435025  \n",
      "4                1             4.100               4.855351  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [4/6]: 100%|██████████| 563/563 [00:14<00:00, 38.56it/s]\n",
      "Validation epoch [4/6]: 100%|██████████| 63/63 [00:01<00:00, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [4/6]:\n",
      "  Loss (Relatedness): 0.2622\n",
      "  Loss (Entailment): 0.5936\n",
      "  Spearman Correlation: 0.7748\n",
      "  Accuracy: 0.8690\n",
      "  F1 Score: 0.8251\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 2274, 2336, 2024, 3061, 1999, 2392, 1997...           0   \n",
      "1  [101, 1037, 2611, 1999, 2317, 2003, 5613, 102,...           1   \n",
      "2  [101, 1037, 2177, 1997, 10158, 2024, 13039, 20...           0   \n",
      "3  [101, 1996, 2450, 4147, 3165, 6471, 1010, 5061...           0   \n",
      "4  [101, 1037, 2450, 2003, 2635, 2125, 1037, 1196...           2   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                1               4.2               4.829823  \n",
      "1                0               4.9               4.128891  \n",
      "2                1               4.1               4.762861  \n",
      "3                2               3.3               3.472594  \n",
      "4                0               3.5               3.205946  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [5/6]: 100%|██████████| 563/563 [00:14<00:00, 38.53it/s]\n",
      "Validation epoch [5/6]: 100%|██████████| 63/63 [00:01<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [5/6]:\n",
      "  Loss (Relatedness): 0.2754\n",
      "  Loss (Entailment): 0.8113\n",
      "  Spearman Correlation: 0.7683\n",
      "  Accuracy: 0.8433\n",
      "  F1 Score: 0.8101\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 2176, 2336, 2024, 2725, 2067, 10609, 510...           0   \n",
      "1  [101, 2274, 2336, 2024, 3061, 1999, 2392, 1997...           0   \n",
      "2  [101, 2619, 2003, 2006, 1037, 2304, 1998, 2317...           0   \n",
      "3  [101, 2019, 2214, 1010, 2327, 3238, 2450, 2003...           0   \n",
      "4  [101, 1037, 2829, 1998, 2317, 3899, 2003, 2770...           0   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                1             3.800               4.167154  \n",
      "1                1             4.200               4.488229  \n",
      "2                1             3.165               4.124984  \n",
      "3                1             3.800               4.538031  \n",
      "4                1             4.400               4.389619  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [6/6]: 100%|██████████| 563/563 [00:14<00:00, 38.51it/s]\n",
      "Validation epoch [6/6]: 100%|██████████| 63/63 [00:01<00:00, 43.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch [6/6]:\n",
      "  Loss (Relatedness): 0.2754\n",
      "  Loss (Entailment): 0.5778\n",
      "  Spearman Correlation: 0.7653\n",
      "  Accuracy: 0.8790\n",
      "  F1 Score: 0.8335\n",
      "Top 5 Misclassified Examples:\n",
      "                                             premise  true_label  \\\n",
      "0  [101, 2274, 2336, 2024, 3061, 1999, 2392, 1997...           0   \n",
      "1  [101, 2048, 6077, 2024, 2652, 2011, 1037, 3392...           1   \n",
      "2  [101, 1037, 2611, 1999, 2317, 2003, 5613, 102,...           1   \n",
      "3  [101, 1037, 2177, 1997, 10158, 2024, 13039, 20...           0   \n",
      "4  [101, 1996, 2450, 4147, 3165, 6471, 1010, 5061...           0   \n",
      "\n",
      "   predicted_label  relatedness_true  relatedness_predicted  \n",
      "0                1               4.2               4.497460  \n",
      "1                0               4.6               4.215297  \n",
      "2                0               4.9               4.093045  \n",
      "3                1               4.1               4.594283  \n",
      "4                2               3.3               3.182624  \n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "    # Write your code here\n",
    "    # train your model\n",
    "    # clear gradient\n",
    "    # forward pass\n",
    "    # compute loss\n",
    "    # back-propagation\n",
    "    # model optimization\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()  # remove grad.\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "        labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        relatedness_score, entailment_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "        loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "        loss = loss_relatedness + loss_entailment\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Write your code here\n",
    "    total_loss_relatedness = 0\n",
    "    total_loss_entailment = 0\n",
    "    total_spc = 0\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    num_batches = 0\n",
    "    error_analysis = []\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_relatedness = batch['labels_relatedness'].to(device)\n",
    "            labels_entailment = batch['labels_entailment'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            relatedness_score, entailment_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_relatedness = loss_fn_relatedness(relatedness_score, labels_relatedness)\n",
    "            loss_entailment = loss_fn_entailment(entailment_logits, labels_entailment)\n",
    "            total_loss_relatedness += loss_relatedness.item()\n",
    "            total_loss_entailment += loss_entailment.item()\n",
    "\n",
    "            # Compute metrics\n",
    "            total_spc += spc(relatedness_score, labels_relatedness).item()\n",
    "            total_acc += acc(entailment_logits, labels_entailment).item()\n",
    "            total_f1 += f1(entailment_logits, labels_entailment).item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Error analysis\n",
    "            predicted_labels = torch.argmax(entailment_logits, dim=1)\n",
    "            for i in range(len(labels_entailment)):\n",
    "                if predicted_labels[i] != labels_entailment[i]:\n",
    "                    error_analysis.append({\n",
    "                        'premise': batch['input_ids'][i].cpu().numpy().tolist(),\n",
    "                        'true_label': labels_entailment[i].item(),\n",
    "                        'predicted_label': predicted_labels[i].item(),\n",
    "                        'relatedness_true': labels_relatedness[i].item(),\n",
    "                        'relatedness_predicted': relatedness_score[i].item()\n",
    "                    })\n",
    "    # Evaluate your model\n",
    "    avg_loss_relatedness = total_loss_relatedness / num_batches\n",
    "    avg_loss_entailment = total_loss_entailment / num_batches\n",
    "    avg_spc = total_spc / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    avg_f1 = total_f1 / num_batches\n",
    "\n",
    "    print(f\"Validation Results - Epoch [{ep+1}/{epochs}]:\")\n",
    "    print(f\"  Loss (Relatedness): {avg_loss_relatedness:.4f}\")\n",
    "    print(f\"  Loss (Entailment): {avg_loss_entailment:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {avg_spc:.4f}\")\n",
    "    print(f\"  Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"  F1 Score: {avg_f1:.4f}\")\n",
    "    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "\n",
    "    if len(error_analysis) > 0:\n",
    "        df_error_analysis = pd.DataFrame(error_analysis)\n",
    "        print(\"Top 5 Misclassified Examples:\")\n",
    "        print(df_error_analysis.head(5))\n",
    "\n",
    "    torch.save(model, f'./saved_models/ep{ep}.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
